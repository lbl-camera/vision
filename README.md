<div id="top"></div>

<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
      <ul>
        <li><a href="#built-with">Built With</a></li>
      </ul>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#installation">Installation</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#roadmap">Roadmap</a></li>
    <li><a href="#contributing">Contributing</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
    <li><a href="#acknowledgments">Acknowledgments</a></li>
  </ol>
</details>


# Vision
This page contains a list of tutorials about computer vision algorithms, visualization pipelines, multiple views from image data as created <b> by Daniela Ushizima</b> in collaboration with members of [CAMERA](https://camera.lbl.gov/).

## Registration, CNNs, and 3D segmentations

<table border="0">
 <tr>
    <td><img src="https://github.com/dani-lbnl/introvision/blob/main/cameracomputervision.png" width="1300">
    </td>
    <td>
     <p>
      DOE research across a myriad of science domains are reliant on image-based data from experiments. This work contains machine learning algorithms based on CNNs such as U-net, Tiramisu, Y-net for semantic segmentation, and many more for image classification, such as VGGs, ResNets, Xception. These have been applied to different scientific problems, where information was hidden in data from X-ray and neutron-based imaging, and electron microscopy. The CAMERA Computer Vision team and collaborators have worked together toward delivering a new modus operandi for analyzing results of experiments conducted at LBNL and other DOE facilities, providing insight to guide and optimize experiments. To better exploit the scientific value of a broad array of high resolution, multidimensional datasets, we have created scientific procedures based on dask, jupyter, itk, xarray, zarr, itkwidgets, Tensorflow, pytorch to measure microstructures  and track materials deformation during lapsing experiments. In addition, we have explored Natural Language Processing (NLP) frameworks such as NLK, Spacy, BERTopic to augment image-based knowledge with topics mined from literature. Some of these applications are discussed below.  
      </td>
 </tr>
</table>

## Tutorials with code
- Lecture at [Zeiss Berkeley Brain Microscopy Innovation Center, UC Berkeley](https://github.com/dani-lbnl/2017_ucberkeley_course)
- Run skimage, sklearn [jupyter notebooks](https://github.com/dani-lbnl/isvc2019) locally or using Google Colab
- How to use [dask, jupyter, itk, xarray, zarr, itkwidgets](https://github.com/dani-lbnl/SC20_pyHPC) to inspect and segment microtomography images
- Convert data types for plant root analysis [rsml2image](https://github.com/dani-lbnl/rsml2image)
- U-nets for [lung](https://www.nature.com/articles/s41598-021-95561-y) segmentation and [ceramic matrix composites](https://www.nature.com/articles/s41597-022-01119-6) 
- Experiments using public data sets [gdrive](https://drive.google.com/drive/folders/19YiBCQh4Z1LB9iCVWfDj4Zfx-zn-VD16?usp=sharing)

<!-- ABOUT THE PROJECT -->
<!--## About The Project-->

## Acknowledgements:
- Alex Hexemer
- Hari Krishnan
- Mudit Mangal
- Silvia Miramontes
- Dula Parkinson
- David Perlmutter
- Jerome Quenum
- James Sethian
- Alexandre Siqueira
- Stephan van der Walt



